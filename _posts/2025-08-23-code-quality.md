---
layout: post
title: "Enforcing Code Quality with Agential Development"
date: 2025-08-23 07:28:00 -0700
categories: [Process, Code Quality]
tags: [ai-development, process, code-quality, agents]
---



## Introduction

One thing I've been trying to figure out is howâ€”or whetherâ€”any form of code quality can be enforced on the model and agents, particularly beyond requiring human reviewer involvement. The initial quality gates have been simple, static tools such as type checkers, linters, and tests. In addition to those tools, there's a `code-reviewer` agent with specific requirementsâ€”a checklist that all submissions must satisfy before the agent will approve them. When submissions don't meet these requirements, the `code-reviewer` agent kicks the changes back to the model/developer agents to fix the issues raised.

## Extending Code Quality Enforcement 

That works okay, but I wanted to see if it could be further improved. So I discussed with the model both automated code quality measurement systems and existing methodologies for measuring code quality. On the automated side, you have static analysis tools, and we'll likely eventually implement an MCP server running some of them to perform code checks. On the methodology side, the model suggested several different approaches, and from those discussions, four new agents were created: `clean-code-analyst`, `architectural-patterns-expert`, `maintainability-assessor`, and `solid-principles-assessor`. These four agents evaluate the codebase using their respective methodologies, and when they identify specific issues in the code, they perform two actions: 1. Mark the offending piece of code with a comment containing a UUID-based tech debt marker, and 2. Add the marker with UUID into a tracking file. There's also a set of command-line tools to quickly generate these comments, remediate the items, and scan the codebase for such markers.

The current process has the model and agents work on a task, after which the `code-reviewer` agent reviews the changes, typically at the user story level. Then, at the sprint level, the four assessor agents evaluate the code based on their respective criteria. When the assessors identify issues with the code, the work gets returned to the model and agents to either fix the problems or restart the sprint with a new plan. One interesting aspect is that using tech debt markers and maintaining a permanent record of them enables us to establish metrics about the types of issues the model and agents encounter in coding situations. Obviously, there's the limitation that we're only tracking issues the assessors are designed to identify, but it's a meaningful starting point.

## Code Quality Enforcement In Practice: An Example

A good example of this process in action occurred last night with my kernel-tools project, which represents the current iteration of what were originally shell-like Python programs designed to help with my RHEL kernel subsystem maintenance work. This was the initial codebase I targeted when starting this AI development journey and has been refactored and evolved several times, transitioning from three separate programs into a monorepo with a shared library of common code. The assessors were directed to evaluate the repository and given free reinâ€”with brutally honest results. While the model tends to rationalize and praise the code quality, the assessors were having none of it. I initially directed them to evaluate a couple of other projects they've been largely responsible for creating, and those grades tended to fall in the B+ range. The kernel-tools project, which began as three separate commands developed over several years, still carries technical debt from that legacy, and the assessors had a field day with it. ðŸ˜‚ The grades averaged somewhere in the D+/C- range. They had a list of improvements that could be made, though. So now the model and agents had a roadmap.

So a TDD/agile plan was developed, and the model and agents began to execute it. They eventually reached the point of implementing a dependency injection system. First attempt: `senior-engineer` took a shot at it and failed, with `code-reviewer` determining that they were "gaming" the constraints system to pass tests. Second attempt: `senior-engineer` tried again and failed once more. The `senior-engineer`, who had been inconsistent with other recent workâ€”possibly due to corruption in their prompt file from an attempt by the model to bulk edit the agent promptsâ€”was retired from the project with the model's approval. ðŸ˜€ The third attempt was assigned to `python-cli-specialist`, who found yet another way to fail. The `code-reviewer` dutifully rejected it again, actually expressing "frustration" in their process_thoughts sent to the private journal. Clearly, they were making no progress. The assessors were brought back in to evaluate the situation and develop a path forward. Two additional agents were tasked with creating a detailed plan: `systems-architect` and `api-design-expert`. At this point, `python-cli-specialist` returned to the task armed with a solid plan from the assessors and planning agentsâ€”and failed to meet the quality bar once more. So we paused to consider how to get past this roadblock. ðŸ’¡Of course, the answer followed the pattern of the past 3-4 weeksâ€”enter `python-dependency-injection-specialist`. This agent developed a five-phase plan, with the code-reviewer and assessors evaluating progress after each phase, enabling incremental progress toward the goal. `python-dependency-injection-specialist` succeeded on phase 1 in their first attempt. They breezed through phase 2 as well. Phase 3 received pushback regarding documentation that described the status incorrectlyâ€”it didn't match the actual code, according to `code-reviewer`. That issue was resolved, and they progressed to phase 4. 

Eventually, they completed phase 5 and began working on cleaning up additional issues. Once that work is complete, we'll have the four assessors perform another assessment of the project state to see the updated grades, plus generate a table showing the progression of grades from each assessor across different code quality metrics.

It was fascinating to watch this process unfold, though it required some guidance and prodding on my part to get the agents to consult the assessors about potential solutions and enlist the support of `systems-architect` and `api-design-expert` to create a detailed step-by-step implementation plan. I don't know how long the model would have continued going in circles trying to solve the issue, but it doesn't need to when there's a human available to help navigate such roadblocks and guide it toward a solution.

The model has suggested additional assessor agents, so these will be added to our stable as well to see what they contribute and how effectively they perform. This approach provides an interesting complement to traditional static analysis tools, identifying and addressing issues that those automated tools would never detect.
